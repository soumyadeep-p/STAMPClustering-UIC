{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:18:46.075690Z","iopub.execute_input":"2024-11-09T21:18:46.076029Z","iopub.status.idle":"2024-11-09T21:18:49.523169Z","shell.execute_reply.started":"2024-11-09T21:18:46.075994Z","shell.execute_reply":"2024-11-09T21:18:49.522239Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Loading STL-10 Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom PIL import Image\n\n# Define transformations\n# transform = transforms.Compose([\n#     transforms.ToTensor(),\n#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n# ])\n\n# Define the specific transformations for CLIP\n\n# Define the specific transformations for CLIP\nclip_transform = transforms.Compose([\n    transforms.Resize(256),               # Resize the shorter side to 256 while maintaining aspect ratio\n    transforms.CenterCrop(224),            # Crop the center to 224x224 size (CLIP expects 224x224 images)\n    transforms.ToTensor(),                 # Convert the image to a PyTorch tensor\n    transforms.Normalize(                 # Normalize the image using CLIP's mean and standard deviation\n        mean=[0.48145466, 0.4578275, 0.40821073],  # CLIP's mean for RGB channels\n        std=[0.26862954, 0.26130258, 0.27577711]    # CLIP's std for RGB channels\n    ),\n])\n\n# Load STL-10 dataset (train and test sets)\ntrain_dataset = datasets.STL10(root='/kaggle/working', split='train', download=True, transform=clip_transform)\ntest_dataset = datasets.STL10(root='/kaggle/working', split='test', download=True, transform=clip_transform)\n\n# Split train dataset into train and validation sets for unsupervised learning\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\nbatch_size = 256\n\n# Create data loaders for each set\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:21:19.839899Z","iopub.execute_input":"2024-11-09T21:21:19.840688Z","iopub.status.idle":"2024-11-09T21:25:02.800976Z","shell.execute_reply.started":"2024-11-09T21:21:19.840647Z","shell.execute_reply":"2024-11-09T21:25:02.800025Z"}},"outputs":[{"name":"stdout","text":"Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to /kaggle/working/stl10_binary.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2640397119/2640397119 [03:01<00:00, 14578035.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting /kaggle/working/stl10_binary.tar.gz to /kaggle/working\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Loading Pre-trained ViT using Dino technique","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch.hub\n\n# Load DINO pre-trained model from Facebook AI repository\ndino_model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n\n# Set to evaluation mode\ndino_model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:29:34.667622Z","iopub.execute_input":"2024-11-09T21:29:34.668545Z","iopub.status.idle":"2024-11-09T21:29:36.767341Z","shell.execute_reply.started":"2024-11-09T21:29:34.668499Z","shell.execute_reply":"2024-11-09T21:29:36.766514Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /root/.cache/torch/hub/main.zip\nDownloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n100%|██████████| 82.7M/82.7M [00:00<00:00, 216MB/s] \n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (blocks): ModuleList(\n    (0-11): 12 x Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n  )\n  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n  (head): Identity()\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom tqdm import tqdm  # For progress bar\nimport json\n\ndef extract_dino_features_and_labels(dino_model, dataloader):\n    dino_model.to(device)\n    dino_model.eval()\n    all_features = []\n    all_labels = []\n    with torch.no_grad():\n        for images, labels in tqdm(dataloader, desc=\"Extracting DINO features\"):\n            images = images.to(device)  # Move images to GPU\n            # print(images.shape)\n            # Extract features\n            features = dino_model(images)  # Assuming DINO returns a feature vector for each image\n            features = features / features.norm(dim=-1, keepdim=True)  # Normalize the features (if needed)\n            all_features.append(features.cpu())  # Store features on CPU\n            all_labels.append(labels.numpy())\n\n    # Concatenate all features from the batch\n    all_features = torch.cat(all_features, dim=0)  # Shape: (num_samples, feature_dim)\n    all_labels = np.hstack(all_labels)\n    return all_features, all_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:29:39.979286Z","iopub.execute_input":"2024-11-09T21:29:39.979749Z","iopub.status.idle":"2024-11-09T21:29:40.739337Z","shell.execute_reply.started":"2024-11-09T21:29:39.979703Z","shell.execute_reply":"2024-11-09T21:29:40.738316Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Processing training set","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n\n# Path to the processed data file\nprocessed_data_path = '/kaggle/input/stl-10-processed/train_data.pt'\n\n# Check if the file exists\nif os.path.exists(processed_data_path):\n    # Load the preprocessed data\n    checkpoint = torch.load(processed_data_path)\n    train_features = checkpoint['train_features']\n    train_labels = checkpoint['train_labels']\n    print(\"Loaded train_features and train_labels from train_data.pt.\")\nelse:\n    # Extract features and labels using the DINO model\n    train_features, train_labels = extract_dino_features_and_labels(dino_model, train_loader)\n\n    # Save the data for future use\n    torch.save({\n        'train_features': train_features,\n        'train_labels': torch.tensor(train_labels)\n    }, 'train_data.pt')\n    print(\"Extracted features and labels and saved to train_data.pt.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:29:44.841289Z","iopub.execute_input":"2024-11-09T21:29:44.841818Z","iopub.status.idle":"2024-11-09T21:29:55.377252Z","shell.execute_reply.started":"2024-11-09T21:29:44.841753Z","shell.execute_reply":"2024-11-09T21:29:55.376106Z"}},"outputs":[{"name":"stderr","text":"Extracting DINO features: 100%|██████████| 16/16 [00:10<00:00,  1.55it/s]","output_type":"stream"},{"name":"stdout","text":"Extracted features and labels and saved to train_data.pt.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Processing test set","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n\n# Path to the processed data file\nprocessed_data_path = '/kaggle/input/stl-10-processed/test_data.pt'\n\n# Check if the file exists\nif os.path.exists(processed_data_path):\n    # Load the preprocessed data\n    checkpoint = torch.load(processed_data_path)\n    test_features = checkpoint['test_features']\n    test_labels = checkpoint['test_labels']\n    print(\"Loaded test_features and test_labels from test_data.pt.\")\nelse:\n    # Extract features and labels using the DINO model\n    test_features, test_labels = extract_dino_features_and_labels(dino_model, test_loader)\n\n    # Save the data for future use\n    torch.save({\n        'test_features': test_features,\n        'test_labels': torch.tensor(test_labels)\n    }, 'test_data.pt')\n    print(\"Extracted test_features and test_features and saved to test_data.pt.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:29:57.180559Z","iopub.execute_input":"2024-11-09T21:29:57.181474Z","iopub.status.idle":"2024-11-09T21:30:14.924642Z","shell.execute_reply.started":"2024-11-09T21:29:57.181419Z","shell.execute_reply":"2024-11-09T21:30:14.923586Z"}},"outputs":[{"name":"stderr","text":"Extracting DINO features: 100%|██████████| 32/32 [00:17<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracted test_features and test_features and saved to test_data.pt.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Processing val set","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n\n# Path to the processed data file\nprocessed_data_path = '/kaggle/input/stl-10-processed/val_data.pt'\n\n# Check if the file exists\nif os.path.exists(processed_data_path):\n    # Load the preprocessed data\n    checkpoint = torch.load(processed_data_path)\n    val_features = checkpoint['val_features']\n    val_labels = checkpoint['val_labels']\n    print(\"Loaded val_features and val_labels from val_data.pt.\")\nelse:\n    # Extract features and labels using the DINO model\n    val_features, val_labels = extract_dino_features_and_labels(dino_model, val_loader)\n\n    # Save the data for future use\n    torch.save({\n        'val_features': val_features,\n        'val_labels': torch.tensor(val_labels)\n    }, 'val_data.pt')\n    print(\"Extracted features and labels and saved to val_data.pt.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:30:20.494322Z","iopub.execute_input":"2024-11-09T21:30:20.495339Z","iopub.status.idle":"2024-11-09T21:30:23.656731Z","shell.execute_reply.started":"2024-11-09T21:30:20.495279Z","shell.execute_reply":"2024-11-09T21:30:23.655452Z"}},"outputs":[{"name":"stderr","text":"Extracting DINO features: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]","output_type":"stream"},{"name":"stdout","text":"Extracted features and labels and saved to val_data.pt.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Functions to generate caption subjects corresponding to images","metadata":{}},{"cell_type":"code","source":"import torch\n# from transformers import processor\nfrom torchvision import transforms\n\nimport torch\nimport numpy as np\nfrom tqdm import tqdm  # For progress bar\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nimport spacy\nfrom torchvision import transforms\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cluster import DBSCAN\nfrom collections import Counter\n\n\n# Load models\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nblip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ndef generate_captions(images, model, processor, device):\n    \"\"\"Generate captions for a batch of images using BLIP.\"\"\"\n    unnormalize = transforms.Normalize((-1, -1, -1), (2, 2, 2))\n\n    # Unnormalize and convert to PIL images for the batch\n    images = unnormalize(images)\n    images = [transforms.ToPILImage()(img) for img in images]\n\n    # Prepare the inputs using the processor for the entire batch\n    inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n\n    # Generate captions for the batch\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=10)\n    \n    # Decode captions for all images in the batch\n    captions = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n    return captions\n\ndef extract_subjects(captions, nlp):\n    \"\"\"Extract the main subject from each caption.\"\"\"\n    subjects = []\n    for caption in captions:\n        doc = nlp(caption)\n        subject = \"outlier\"  # Default subject\n        for token in doc:\n            if token.dep_ in {\"nsubj\", \"nsubjpass\"}:  # Find subject\n                subject = token.text\n                break\n\n        if subject == \"outlier\":\n            for token in doc:\n                if token.dep_ == \"ROOT\":  # Backup: Find the root\n                    subject = token.text\n                    break\n        subjects.append(subject)\n    return subjects\n\n\ndef extract_subjects_batch(images):\n    \"\"\"Main function to extract features from images using BLIP and CLIP.\"\"\"\n    captions = generate_captions(images, blip_model, blip_processor, device)\n    subjects = extract_subjects(captions, nlp)\n    return subjects\n\ndef extract_pseudolabels(data_loader):\n    \"\"\"Extract features and labels for an entire dataset.\"\"\"\n    subjects = []\n    with torch.no_grad():\n        for images, labels in tqdm(data_loader, desc=\"Extracting features and labels\"):\n            images = images.to(device)  # Move images to GPU\n            subject = extract_subjects_batch(images)  # Extract features\n            subjects.extend(subject)\n\n    return subjects\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:30:41.889310Z","iopub.execute_input":"2024-11-09T21:30:41.889739Z","iopub.status.idle":"2024-11-09T21:31:06.818757Z","shell.execute_reply.started":"2024-11-09T21:30:41.889695Z","shell.execute_reply":"2024-11-09T21:31:06.817856Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40a565fe24984054a5efdc2b51a1bdd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8188cc9ae14c4405bce78920cb09ae96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53f7df11f6414b109ad67742835879b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f4351165d3450ba1c53aeaf10999dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8800b54764fa4d04aa603c96f07c1481"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5d34cf81fdc42988d4dd4999fd790a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5d6adb6cc3847a69ee0f1ab2ec2765b"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Generating subjects and storing ","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nfrom collections import Counter\n\n# Path to the JSON file\nsubjects_path = '/kaggle/input/stl-10-processed/subjects.json'\n\n# Check if the JSON file exists\nif os.path.exists(subjects_path):\n    # Load subjects from the JSON file\n    with open(subjects_path, 'r') as file:\n        subjects = json.load(file)\n    print(\"subjects loaded from subjects.json:\", Counter(subjects))\nelse:\n    # Extract pseudolabels\n    subjects = extract_pseudolabels(train_loader)\n    \n    # Save subjects to a JSON file\n    with open('subjects.json', 'w') as file:\n        json.dump(subjects, file)\n    print(\"subjects saved to subjects.json.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-09T21:31:06.820604Z","iopub.execute_input":"2024-11-09T21:31:06.821244Z","iopub.status.idle":"2024-11-09T21:35:26.362744Z","shell.execute_reply.started":"2024-11-09T21:31:06.821206Z","shell.execute_reply":"2024-11-09T21:35:26.361736Z"}},"outputs":[{"name":"stderr","text":"Extracting features and labels:   0%|          | 0/16 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nExtracting features and labels: 100%|██████████| 16/16 [04:19<00:00, 14.62s/it]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nExtracting features and labels: 100%|██████████| 16/16 [04:19<00:00, 16.22s/it]","output_type":"stream"},{"name":"stdout","text":"subjects saved to subjects.json.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10}]}